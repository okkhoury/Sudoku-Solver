{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import load_model\n",
    "import sklearn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#I did not write this code. It was provided in the kaggle page\n",
    "#https://www.kaggle.com/bryanpark/sudoku?sortBy=null&group=datasets\n",
    "quizzes = np.zeros((1000000, 81), np.int32)\n",
    "solutions = np.zeros((1000000, 81), np.int32)\n",
    "for i, line in enumerate(open('sudoku.csv', 'r').read().splitlines()[1:]):\n",
    "    quiz, solution = line.split(\",\")\n",
    "    for j, q_s in enumerate(zip(quiz, solution)):\n",
    "        q, s = q_s\n",
    "        quizzes[i, j] = q\n",
    "        solutions[i, j] = s\n",
    "#quizzes = quizzes.reshape((-1, 9, 9))\n",
    "#solutions = solutions.reshape((-1, 9, 9))\n",
    "# make a train test split\n",
    "#quizzes are inputs\n",
    "#conver the outputs to be categorical outputs\n",
    "\n",
    "#y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#quizzes = np.expand_dims(quizzes, -1)\n",
    "#quizzes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try changing the data types to float32 it might get faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 6, 4, 3, 7, 1, 2, 5, 9, 3, 2, 5, 8, 4, 9, 7, 6, 1, 9, 7, 1, 2, 6,\n",
       "       5, 8, 4, 3, 4, 3, 6, 1, 9, 2, 5, 8, 7, 1, 9, 8, 6, 5, 7, 4, 3, 2, 2,\n",
       "       5, 7, 4, 8, 3, 9, 1, 6, 6, 8, 9, 7, 3, 4, 1, 2, 5, 7, 1, 3, 5, 2, 8,\n",
       "       6, 9, 4, 5, 4, 2, 9, 1, 6, 3, 7, 8])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solutions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solutions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "quiz_train, quiz_test, output_train, output_test = sklearn.model_selection.train_test_split(quizzes, solutions, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Build a Basic MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_110 (Dense)            (None, 256)               20992     \n",
      "_________________________________________________________________\n",
      "batch_normalization_87 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_88 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_89 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_90 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_91 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 81)                20817     \n",
      "=================================================================\n",
      "Total params: 310,097.0\n",
      "Trainable params: 307,537\n",
      "Non-trainable params: 2,560.0\n",
      "_________________________________________________________________\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/3\n",
      "1000000/1000000 [==============================] - 296s - loss: 4.7881 - acc: 0.0752 - val_loss: 3.7484 - val_acc: 0.0823\n",
      "Epoch 2/3\n",
      "1000000/1000000 [==============================] - 300s - loss: 3.8775 - acc: 0.0870 - val_loss: 3.5816 - val_acc: 0.1059\n",
      "Epoch 3/3\n",
      "1000000/1000000 [==============================] - 299s - loss: 3.7716 - acc: 0.0910 - val_loss: 3.4905 - val_acc: 0.0940\n",
      " 999872/1000000 [============================>.] - ETA: 0sTest loss: 3.49049021709\n",
      "Test accuracy: 0.093992\n"
     ]
    }
   ],
   "source": [
    "#need 729 outputs 81 cells. 81 possible probabilities\n",
    "mlp = Sequential()\n",
    "mlp.add(Dense(256, activation = 'relu', input_shape = (81,)))\n",
    "mlp.add(BatchNormalization())\n",
    "mlp.add(Dense(256, activation = 'relu'))\n",
    "mlp.add(BatchNormalization())\n",
    "mlp.add(Dense(256, activation = 'relu'))\n",
    "mlp.add(BatchNormalization())\n",
    "mlp.add(Dense(256, activation = 'relu'))\n",
    "mlp.add(BatchNormalization())\n",
    "mlp.add(Dense(256, activation = 'relu'))\n",
    "mlp.add(BatchNormalization())\n",
    "mlp.add(Dense(81, activation = 'relu'))\n",
    "\n",
    "mlp.summary()\n",
    "\n",
    "mlp.compile(loss='mean_squared_error', optimizer=RMSprop(), \n",
    "            metrics=['accuracy'])\n",
    "\n",
    "history = mlp.fit(quizzes[0:500000], solutions[0:500000], batch_size = 100, epochs = 3, \n",
    "                  verbose = 1, validation_data = (quizzes[500000:600000,solutions[500000:600000]))\n",
    "score = mlp.evaluate(quizzes, solutions, verbose = 1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Try A Basic CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = quizzes.reshape(quizzes.shape[0], 81, 1)\n",
    "#X_test = X_test.reshape(X_test.shape[0], img_cols, img_rows, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/3\n",
      "1000000/1000000 [==============================] - 3147s - loss: 3.4106 - acc: 0.0889 - val_loss: 3.0947 - val_acc: 0.0986\n",
      "Epoch 2/3\n",
      " 215700/1000000 [=====>........................] - ETA: 1726s - loss: 3.1296 - acc: 0.0968"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-2a93d25c2aa3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m           validation_data=(X_train, solutions))\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolutions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test loss:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Pawn\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    843\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 845\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    846\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\Users\\Pawn\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1485\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1487\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Pawn\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1140\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1141\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Pawn\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2073\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2074\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2075\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Pawn\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Pawn\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Pawn\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\Pawn\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Pawn\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cnn = Sequential()cnn.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(81,1)))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Conv1D(32, kernel_size=3, activation='relu'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Conv1D(32, kernel_size=3, activation='relu'))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Dense(81, activation = 'linear'))\n",
    "#cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#cnn.add(Flatten())\n",
    "#cnn.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "cnn.compile(loss='mean_squared_error',\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history2 = cnn.fit(X_train,solutions,\n",
    "          batch_size=100,\n",
    "          epochs=3,\n",
    "          verbose=1,\n",
    "          validation_data=(X_train, solutions))\n",
    "score = cnn.evaluate(X_train, solutions, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp.save('sudokuMLP2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp.save('sudokuMLP.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn.save('sudokuCNN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Try a Deeper CNN with less Iterations per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500000 samples, validate on 100000 samples\n",
      "Epoch 1/1\n",
      "500000/500000 [==============================] - 2744s - loss: 3.6823 - acc: 0.0708 - val_loss: 3.1559 - val_acc: 0.1048\n",
      "100000/100000 [==============================] - 214s   \n",
      "Test loss: 3.15591818687\n",
      "Test accuracy: 0.10479\n"
     ]
    }
   ],
   "source": [
    "cnn2 = Sequential()\n",
    "cnn2.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(81,1)))\n",
    "cnn2.add(BatchNormalization())\n",
    "cnn2.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "cnn2.add(BatchNormalization())\n",
    "cnn2.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "cnn2.add(BatchNormalization())\n",
    "cnn2.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "cnn2.add(BatchNormalization())\n",
    "cnn2.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "cnn2.add(BatchNormalization())\n",
    "cnn2.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "cnn2.add(BatchNormalization())\n",
    "cnn2.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "cnn2.add(BatchNormalization())\n",
    "cnn2.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "cnn2.add(BatchNormalization())\n",
    "cnn2.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "cnn2.add(Flatten())\n",
    "cnn2.add(BatchNormalization())\n",
    "cnn2.add(Dense(81, activation = 'linear'))\n",
    "#cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#cnn.add(Flatten())\n",
    "#cnn.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "cnn2.compile(loss='mean_squared_error',\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history2 = cnn2.fit(X_train[0:500000],solutions[0:500000],\n",
    "          batch_size=100,\n",
    "          epochs=1,\n",
    "          verbose=1,\n",
    "          validation_data=(X_train[500000:600000], solutions[500000:600000]))\n",
    "score2 = cnn2.evaluate(X_train[500000:600000], solutions[500000:600000], verbose=1)\n",
    "print('Test loss:', score2[0])\n",
    "print('Test accuracy:', score2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn2.save(\"deepSudokuCNN.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Make an Iterative Solver for the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81,)\n"
     ]
    }
   ],
   "source": [
    "testPuzzle = quizzes[-1]\n",
    "#testPuzzle = np.array([1])\n",
    "zeros = np.where(testPuzzle == 0)[0]\n",
    "#testPuzzle = testPuzzle.reshape((1,) + testPuzzle.shape)\n",
    "#edit the tensor dimensions so that it can be used for the cnn\n",
    "#testPuzzle = testPuzzle.reshape(1, 81, 1)\n",
    "print(testPuzzle.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 9, 8, 4, 7, 1, 6, 2, 5, 1, 2, 6, 3, 8, 5, 4, 7, 9, 7, 4, 5, 6, 2,\n",
       "       9, 8, 3, 1, 6, 5, 7, 8, 1, 3, 9, 4, 2, 9, 1, 4, 7, 6, 2, 5, 8, 3, 8,\n",
       "       3, 2, 9, 5, 4, 1, 6, 7, 4, 8, 9, 5, 3, 7, 2, 1, 6, 2, 6, 3, 1, 9, 8,\n",
       "       7, 5, 4, 5, 7, 1, 2, 4, 6, 3, 9, 8])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testSolution = solutions[-1]\n",
    "testSolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = load_model('deepSudokuCNN.h5')\n",
    "#np.transpose(testPuzzle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = load_model('sudokuMLP.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81,)\n",
      "(81,)\n"
     ]
    }
   ],
   "source": [
    "print(quizzes[-1].shape)\n",
    "print(testPuzzle.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 81)\n",
      "[[  3.5401125    5.72158146   6.50185347   3.8828671    5.95534897\n",
      "    4.10434341   6.63568544   2.17235756   6.17044353   3.75974274\n",
      "    4.00889921   6.09584236   2.73304057   8.48139      6.75350094\n",
      "    4.3255477    4.39978123   4.05042887   6.48901224   3.91274762\n",
      "    4.63744736   3.53534222   1.71981168   7.48899508   8.07629108\n",
      "    2.92731833   5.90356302   6.11454678   4.73067474   5.91628075\n",
      "    8.50193882   3.06285739   3.28868794   3.47838187   4.82240391\n",
      "    4.70163584   6.5533886    3.95110679   6.22024536   6.34457493\n",
      "    3.9594121    3.63688636   4.73794365   6.66503668   2.60050392\n",
      "    6.25598335   3.19452167   1.73619318   8.7619915    3.78815675\n",
      "    3.27103472   5.5864296    5.69591713   6.37348461   3.84966779\n",
      "    8.33215809   6.49803162   4.58518553   3.2171905    5.59290028\n",
      "    4.94527817   2.47479534   5.16980267   2.2210536    3.76610446\n",
      "    2.83277059   4.46191978  10.05767155   4.87902832   4.42970276\n",
      "    6.20001841   5.75801277   5.84390402   7.06069756   4.21345949\n",
      "    1.83711433   4.41840506   5.55848598   2.52247715   9.30567932\n",
      "    3.88736486]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 3,  5,  6,  3,  5,  4,  6,  2,  6,  3,  4,  6,  2,  8,  6,  4,  4,\n",
       "         4,  6,  3,  4,  3,  1,  7,  8,  2,  5,  6,  4,  5,  8,  3,  3,  3,\n",
       "         4,  4,  6,  3,  6,  6,  3,  3,  4,  6,  2,  6,  3,  1,  8,  3,  3,\n",
       "         5,  5,  6,  3,  8,  6,  4,  3,  5,  4,  2,  5,  2,  3,  2,  4, 10,\n",
       "         4,  4,  6,  5,  5,  7,  4,  1,  4,  5,  2,  9,  3]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testPuzzle = testPuzzle.reshape((1,) + testPuzzle.shape)\n",
    "print(testPuzzle.shape)\n",
    "print(mlp.predict(testPuzzle))\n",
    "prediction = mlp.predict(testPuzzle)\n",
    "#change the type to int so that you we can evaluate the prediction\n",
    "rounded = np.around(prediction)\n",
    "cast = prediction.astype(int)\n",
    "cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def solve(nn, testBoard, solution, netType):\n",
    "    #into our cnn\n",
    "    # 1:mlp, 2:1d cnn, 3:2d cnn\n",
    "    tensor = None\n",
    "    #depending on the type of net you want to predict with set the tensor dimensions\n",
    "    if netType == 2:\n",
    "        tensor = testBoard.reshape(1, 81, 1)\n",
    "    elif netType == 1:\n",
    "        #print(\"Reshaping the tensor for mlp\")\n",
    "        tensor = testBoard.reshape(1,81)\n",
    "        #print(tensor.shape)\n",
    "    prediction = nn.predict(tensor)\n",
    "    rounded = np.around(prediction)\n",
    "    cast = prediction.astype(int)\n",
    "    correct = 0\n",
    "    if netType == 2 or netType == 1:\n",
    "        for current in range(81):\n",
    "            #compare the values of the cast and the solution\n",
    "            if cast[0][current] == solution[current]:\n",
    "                correct += 1\n",
    "            accuracy = correct / 81\n",
    "    print(cast)\n",
    "    print(\"The accuracy of the board was: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0 0 4 0 1 6 2 0 1 0 0 0 8 0 4 0 0 0 0 5 0 2 0 8 3 0 0 5 7 8 0 0 0 0 0 0\n",
      " 0 0 7 0 0 5 0 3 0 0 2 9 0 4 0 0 7 4 8 0 5 3 0 0 1 0 2 0 3 0 9 0 0 0 0 0 7\n",
      " 0 0 0 6 0 9 0]\n",
      "[[ 3  6  7  3  7  1  5  2  6  1  3  4  4  8  6  3  5  5  7  4  5  3  2  5\n",
      "   7  2  5  6  4  8  7  2  3  3  4  3  6  3  4  7  4  4  4  5  2  6  2  1\n",
      "   8  3  3  5  6  7  4  8  7  3  3  7  4  0  5  2  3  2  3  9  6  5  6  5\n",
      "   5  6  3  2  2  6  3 10  3]]\n",
      "The accuracy of the board was: 0.3333333333333333\n",
      "[[ 3  5  6  3  5  4  6  2  6  3  4  6  2  8  6  4  4  4  6  3  4  3  1  7\n",
      "   8  2  5  6  4  5  8  3  3  3  4  4  6  3  6  6  3  3  4  6  2  6  3  1\n",
      "   8  3  3  5  5  6  3  8  6  4  3  5  4  2  5  2  3  2  4 10  4  4  6  5\n",
      "   5  7  4  1  4  5  2  9  3]]\n",
      "The accuracy of the board was: 0.2345679012345679\n",
      "[3 0 0 4 0 1 6 2 0 1 0 0 0 8 0 4 0 0 0 0 5 0 2 0 8 3 0 0 5 7 8 0 0 0 0 0 0\n",
      " 0 0 7 0 0 5 0 3 0 0 2 9 0 4 0 0 7 4 8 0 5 3 0 0 1 0 2 0 3 0 9 0 0 0 0 0 7\n",
      " 0 0 0 6 0 9 0]\n"
     ]
    }
   ],
   "source": [
    "print(quizzes[-1])\n",
    "solve(nn, quizzes[-1], solutions[-1], 2)\n",
    "solve(mlp, quizzes[-1], solutions[-1], 1)\n",
    "print(quizzes[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#keep going until the there are no more zeros in the input\n",
    "#use the nn to predict the solution\n",
    "#repredict the using the update input\n",
    "def iterative(nn, testBoard, solution, netType):\n",
    "    zeros = np.where(testBoard == 0)[0]\n",
    "    while len(zeros) != 0:\n",
    "        if netType == 2:\n",
    "            tensor = testBoard.reshape(1, 81, 1)\n",
    "        elif netType == 1:\n",
    "            #print(\"Reshaping the tensor for mlp\")\n",
    "            tensor = testBoard.reshape(1,81)\n",
    "            #print(tensor.shape)\n",
    "        prediction = nn.predict(tensor)\n",
    "        rounded = np.around(prediction)\n",
    "        cast = prediction.astype(int)\n",
    "        #update the testboard\n",
    "        #print(test)\n",
    "        #print(zeros[0])\n",
    "        #print(cast[0][zeros[0]])\n",
    "        index = zeros[0]\n",
    "        testBoard[index] = cast[0][index]\n",
    "        #remove the first element from zeros\n",
    "        zeros = np.delete(zeros, [0])\n",
    "    correct = 0\n",
    "    if netType == 2 or netType == 1:\n",
    "        for current in range(81):\n",
    "            #compare the values of the cast and the solution\n",
    "            if cast[0][current] == solution[current]:\n",
    "                correct += 1\n",
    "            accuracy = correct / 81\n",
    "    #print(cast)\n",
    "    print(\"The accuracy of the board was: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0 0 4 0 1 6 2 0 1 0 0 0 8 0 4 0 0 0 0 5 0 2 0 8 3 0 0 5 7 8 0 0 0 0 0 0\n",
      " 0 0 7 0 0 5 0 3 0 0 2 9 0 4 0 0 7 4 8 0 5 3 0 0 1 0 2 0 3 0 9 0 0 0 0 0 7\n",
      " 0 0 0 6 0 9 0]\n",
      "The accuracy of the board was: 0.32098765432098764\n",
      "[3 0 0 4 0 1 6 2 0 1 0 0 0 8 0 4 0 0 0 0 5 0 2 0 8 3 0 0 5 7 8 0 0 0 0 0 0\n",
      " 0 0 7 0 0 5 0 3 0 0 2 9 0 4 0 0 7 4 8 0 5 3 0 0 1 0 2 0 3 0 9 0 0 0 0 0 7\n",
      " 0 0 0 6 0 9 0]\n"
     ]
    }
   ],
   "source": [
    "iterative(mlp, np.copy(quizzes[-1]), solutions[-1], netType = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 6, 7, 4, 7, 1, 6, 2, 6, 1, 3, 5, 4, 8, 7, 4, 6, 6, 7, 4, 5, 3, 2,\n",
       "       7, 8, 3, 5, 7, 5, 7, 8, 1, 3, 4, 5, 5, 5, 4, 3, 7, 6, 4, 5, 6, 3, 4,\n",
       "       3, 2, 9, 4, 4, 6, 7, 7, 4, 8, 6, 5, 3, 6, 4, 1, 5, 2, 4, 3, 3, 9, 5,\n",
       "       6, 7, 5, 5, 7, 4, 2, 4, 6, 5, 9, 2])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quizzes[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 0, 4, 0, 1, 6, 2, 0, 1, 0, 0, 0, 8, 0, 4, 0, 0, 0, 0, 5, 0, 2,\n",
       "       0, 8, 3, 0, 0, 5, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 5, 0, 3, 0,\n",
       "       0, 2, 9, 0, 4, 0, 0, 7, 4, 8, 0, 5, 3, 0, 0, 1, 0, 2, 0, 3, 0, 9, 0,\n",
       "       0, 0, 0, 0, 7, 0, 0, 0, 6, 0, 9, 0])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_43 (Dense)             (None, 128)               10496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 810)               104490    \n",
      "=================================================================\n",
      "Total params: 183,594.0\n",
      "Trainable params: 182,314\n",
      "Non-trainable params: 1,280.0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pawn\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=810)`\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: expected dense_48 to have shape (None, 810) but got array with shape (40500000, 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-9324d8b1bcd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m history = mlp2.fit(quizzes[0:500000], y_train, batch_size = 100, epochs = 3, \n\u001b[1;32m---> 21\u001b[1;33m                   verbose = 1, validation_data = (quizzes[500000:600000], y_test))\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquizzes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m500000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m600000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test loss:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Pawn\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    843\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 845\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    846\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\Users\\Pawn\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1403\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1404\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1405\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1406\u001b[0m         \u001b[1;31m# prepare validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1407\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Pawn\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1297\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1299\u001b[1;33m                                     exception_prefix='model target')\n\u001b[0m\u001b[0;32m   1300\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[0;32m   1301\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[1;32mC:\\Users\\Pawn\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    131\u001b[0m                             \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                             \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m                             str(array.shape))\n\u001b[0m\u001b[0;32m    134\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model target: expected dense_48 to have shape (None, 810) but got array with shape (40500000, 10)"
     ]
    }
   ],
   "source": [
    "#need 729 outputs 81 cells. 81 possible probabilities\n",
    "mlp2 = Sequential()\n",
    "mlp2.add(Dense(128, activation = 'relu', input_shape = (81,)))\n",
    "mlp2.add(BatchNormalization())\n",
    "mlp2.add(Dense(128, activation = 'relu'))\n",
    "mlp2.add(BatchNormalization())\n",
    "mlp2.add(Dense(128, activation = 'relu'))\n",
    "mlp2.add(BatchNormalization())\n",
    "mlp2.add(Dense(128, activation = 'relu'))\n",
    "mlp2.add(BatchNormalization())\n",
    "mlp2.add(Dense(128, activation = 'relu'))\n",
    "mlp2.add(BatchNormalization())\n",
    "mlp2.add(Dense(output_dim = 810, activation = 'softmax'))\n",
    "\n",
    "mlp2.summary()\n",
    "\n",
    "mlp2.compile(loss='categorical_crossentropy', optimizer=RMSprop(), \n",
    "            metrics=['accuracy'])\n",
    "\n",
    "history = mlp2.fit(quizzes[0:500000], y_train, batch_size = 100, epochs = 3, \n",
    "                  verbose = 1, validation_data = (quizzes[500000:600000], y_test))\n",
    "score = mlp2.evaluate(quizzes[500000:600000], y_test, verbose = 1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
